import customtkinter as ctk
from tkinter import scrolledtext
import threading, queue, re, ast, operator, requests, json

# --- UI Configuration ---
ctk.set_appearance_mode("Dark")
ctk.set_default_color_theme("blue")

# ==========================================
# MODULE 1: Deterministic Math Engine
# ==========================================
class MathProcessor:
    """Evaluates strict math to save LLM compute and guarantee 100% accuracy."""
    @staticmethod
    def safe_eval(expr):
        expr = re.sub(r'[^0-9\+\-\*\/\(\)\.]', '', expr)
        if not expr: return None
        ops = {ast.Add: operator.add, ast.Sub: operator.sub, ast.Mult: operator.mul,
               ast.Div: operator.truediv, ast.Pow: operator.pow}
        try:
            node = ast.parse(expr, mode='eval').body
            def _eval(node):
                if isinstance(node, ast.Constant): return node.value
                elif isinstance(node, ast.BinOp):
                    return ops[type(node.op)](_eval(node.left), _eval(node.right))
                elif isinstance(node, ast.UnaryOp):
                    return ops[type(node.op)](_eval(node.operand))
                else: raise TypeError(node)
            return f"The exact calculated answer is {_eval(node)}."
        except Exception:
            return None

# ==========================================
# MODULE 2: API-Driven Brain (Ollama)
# ==========================================
class HarryCore:
    def __init__(self):
        self.history = []
        self.math_engine = MathProcessor()
        # We target the local Ollama server port
        self.api_url = "http://localhost:11434/api/generate"

    def route_query(self, user_input):
        """Routing layer to determine the fastest processing method."""
        # 1. Hardware-level Math Processing
        math_match = re.search(r'\d+\s*[\+\-\*\/]\s*\d+', user_input)
        if math_match:
            answer = self.math_engine.safe_eval(math_match.group(0))
            if answer: return answer

        # 2. LLM Processing via Local API
        return self.generate_text(user_input)

    def generate_text(self, user_input):
        # The System Prompt sets the rigorous engineering persona
        prompt = (
            "You are Harry, a highly advanced AI assistant. You specialize in "
            "Computer Engineering, software architecture (Java/Python), and data structures. "
            "You provide brilliant, concise, and highly accurate answers.\n\n"
        )
        
        # Inject memory context
        for turn in self.history:
            prompt += f"USER: {turn['user']}\nHARRY: {turn['harry']}\n"
        prompt += f"USER: {user_input}\nHARRY:"

        # The JSON payload sent to the Ollama server
        payload = {
            "model": "llama3",  # Ensure you ran 'ollama run llama3' in terminal!
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "num_ctx": 4096 # Expanding the context window to 4k tokens
            }
        }
        
        try:
            # We execute a synchronous POST request to our local AI server
            response = requests.post(self.api_url, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            return data.get("response", "Error: Empty response string.").strip()
            
        except requests.exceptions.ConnectionError:
            return "FATAL ERROR: Could not connect to the Brain. Is the Ollama server running?"
        except Exception as e:
            return f"API Exception: {str(e)}"

    def log_history(self, user, harry):
        self.history.append({"user": user, "harry": harry})
        if len(self.history) > 6: # Increased memory since Llama 3 handles context better
            self.history.pop(0)

# ==========================================
# UI: The Application Layer
# ==========================================
class HarryGPT(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Harry AI â€” Llama-3 Powered Assistant")
        self.geometry("1100x850")

        self.grid_columnconfigure(1, weight=1)
        self.grid_rowconfigure(0, weight=1)

        # Sidebar
        self.sidebar = ctk.CTkFrame(self, width=240, corner_radius=0)
        self.sidebar.grid(row=0, column=0, rowspan=2, sticky="nsew")
        
        self.logo = ctk.CTkLabel(self.sidebar, text="HARRY GPT", font=ctk.CTkFont(size=24, weight="bold"))
        self.logo.pack(pady=30)
        
        self.new_chat_btn = ctk.CTkButton(self.sidebar, text="Wipe Memory", command=self.clear_chat)
        self.new_chat_btn.pack(pady=10, padx=20)

        self.status_var = ctk.StringVar(value="Status: Ready to connect to Ollama")
        self.status_lbl = ctk.CTkLabel(self.sidebar, textvariable=self.status_var, font=("Inter", 11))
        self.status_lbl.pack(side="bottom", pady=20)

        # Chat Interface
        self.chat_display = scrolledtext.ScrolledText(self, state='disabled', wrap='word', 
                                                     bg="#1a1a1a", fg="#ececf1", font=("Inter", 12),
                                                     padx=25, pady=25, borderwidth=0)
        self.chat_display.grid(row=0, column=1, padx=20, pady=20, sticky="nsew")

        # Input Console
        self.input_frame = ctk.CTkFrame(self, fg_color="transparent")
        self.input_frame.grid(row=1, column=1, padx=20, pady=(0, 20), sticky="ew")
        
        self.entry = ctk.CTkTextbox(self.input_frame, height=100, font=("Inter", 12), corner_radius=15)
        self.entry.pack(side='left', fill='x', expand=True, padx=(0, 10))
        
        self.send_btn = ctk.CTkButton(self.input_frame, text="Execute", width=120, height=45, 
                                     command=self.on_send, corner_radius=8)
        self.send_btn.pack(side='right')

        # Initialization
        self.brain = HarryCore()
        self.q = queue.Queue()
        self.after(100, self.process_queue)

    def on_send(self):
        user_input = self.entry.get("1.0", "end").strip()
        if not user_input: return
        
        self.entry.delete("1.0", "end")
        self.update_display("YOU", user_input)
        
        self.status_var.set("Harry is querying Llama-3...")
        threading.Thread(target=self.process_input, args=(user_input,), daemon=True).start()

    def process_input(self, user_input):
        response = self.brain.route_query(user_input)
        self.brain.log_history(user_input, response)
        self.q.put(response)

    def update_display(self, speaker, text):
        self.chat_display.configure(state='normal')
        tag = speaker
        self.chat_display.insert('end', f"{speaker}\n", tag)
        self.chat_display.insert('end', f"{text}\n\n")
        
        if speaker == "YOU":
            self.chat_display.tag_config(tag, foreground="#10a37f", font=("Inter", 11, "bold"))
        else:
            self.chat_display.tag_config(tag, foreground="#5dade2", font=("Inter", 11, "bold"))
            
        self.chat_display.see('end')
        self.chat_display.configure(state='disabled')

    def process_queue(self):
        try:
            while True:
                response = self.q.get_nowait()
                self.update_display("HARRY", response)
                self.status_var.set("Status: Idle")
        except queue.Empty:
            pass
        self.after(100, self.process_queue)

    def clear_chat(self):
        self.chat_display.configure(state='normal')
        self.chat_display.delete("1.0", "end")
        self.chat_display.configure(state='disabled')
        self.brain.history = []
        self.status_var.set("Status: Memory Wiped")

if __name__ == "__main__":
    app = HarryGPT()
    app.mainloop()
